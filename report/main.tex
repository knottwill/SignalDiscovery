\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage[margin=3cm]{geometry}
\setlength{\parskip}{1em}
\usepackage{comment}

\usepackage{graphicx}
\graphicspath{ {../plots/} }

\title{S1: Principals of Data Science - Coursework}
\author{William Knottenbelt, wdk24}

\begin{document}

\maketitle

% \begin{comment}

\section*{Section A}

\subsection*{Part (a)}

The total probability density function, $p$, is properly normalised over $M \in [-\infty, +\infty]$ if it integrates to unity. The integral of $p$ can be written as a weighted sum of signal and background terms:

\begin{equation}
\int_{-\infty}^{+\infty} p \, dM = 
f\int_{-\infty}^{+\infty} s \, dM + 
(1-f)\int_{0}^{+\infty} b \, dM.
\label{eq:integral}
\end{equation}

To show that $p$ is properly normalised we require the identity:
\begin{equation}
\int_{-\infty}^{+\infty} e^{-ax^2} = \sqrt{\frac{\pi}{a}},
\label{eq:exp}
\end{equation}

For the signal term, we have:
\[
\int_{-\infty}^{+\infty} s \, dM =
\int_{-\infty}^{+\infty} \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(M - \mu)^2}{2\sigma^2}\right) \, dM
= \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{+\infty} \exp\left(-\frac{x^2}{2\sigma^2}\right) \, dx,
\]
where we used the substitution $x = M - \mu$. This integral has the form of identity \eqref{eq:exp} with $a = \frac{1}{2\sigma^2}$, hence we see that:
\[
\int_{-\infty}^{+\infty} s \, dM =
\frac{1}{\sigma\sqrt{2\pi}} \sqrt{2\sigma^2\pi} = 1.
\]

For the background term we have:
\[
\int_{0}^{+\infty} b \, dM =
\int_{0}^{+\infty} \lambda e^{-\lambda M} \, dM =
(-e^{-\lambda M}) |_0^{+\infty} = 1
\]

Hence, both the signal and background terms integrate to unity. We can plug this into equation \ref{eq:integral} and we see that the normalisation condition is satisfied:
\[ 
\int_{-\infty}^{+\infty} p \, dM = 
f + (1-f) = 1.
\]

\subsection*{Part (b)}

In general, any probability density function, $g(X)$, can be normalised over an arbitrary range $[\alpha, \beta]$ by multiplying it by the factor $\frac{1}{F(\beta) - F(\alpha)}$, where $F$ is its respective cumulative density function, CDF. To see proof of this, we add normalisation factor, $A$, to $g$. We have:
$$
1 = \int_{\alpha}^{\beta} Ag(M) \, dM 
= A\left(\int_{-\infty}^{\beta} g(M) \, dM - \int_{-\infty}^{\alpha} g(M) \, dM \right)
= A(F(\beta) - F(\alpha))
$$
Hence:
$$
A = \frac{1}{F(\beta) - F(\alpha)}
$$

To ensure that the signal and background distributions contribute the correct fractions ($f, (1-f)$) to the total probability, they must be normalised individually before summing them. The CDF of the normal distribution is $F(X) = \Phi(\frac{X-\mu}{\sigma})$, hence the normalisation factor for the signal distribution is:
$$
\frac{1}{\Phi(\frac{\beta-\mu}{\sigma}) - \Phi(\frac{\alpha-\mu}{\sigma})}
$$

The CDF of the exponential decay distribution is:
$$
F(X) = 
\begin{cases} 
1 - e^{-\lambda X} & \text{for } X \geq 0, \\
0 & \text{for } X < 0.
\end{cases}
$$

Hence, provided that $\alpha, \beta > 0$, the normalisation factor for the background distribution is:
$$
\frac{1}{e^{-\lambda\alpha} - e^{-\lambda \beta}}.
$$

The total probability density function is a weighted sum of the signal and background distributions. Since the signal and background are both normalised, the total PDF is also normalised:
\[ 
\int_{\alpha}^{\beta} p \, dM = 
f\int_{\alpha}^{\beta} s \, dM + 
(1-f)\int_{\alpha}^{\beta} b \, dM
= f + (1-f) = 1.
\]

Assuming $\alpha, \beta > 0$, the full expression is:
\[
p(M) = 
\frac{f}{\Phi(\frac{\beta-\mu}{\sigma}) - \Phi(\frac{\alpha-\mu}{\sigma})}
\frac{1}{\sigma\sqrt{2\pi}}
\exp\left(-\frac{(M - \mu)^2}{2\sigma^2}\right)
+
(1-f)
\frac{\lambda e^{-\lambda M}}
{e^{-\lambda\alpha} - e^{-\lambda\beta}}
.
\]

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{part_d_weighted.png}
\caption{Visualisation of the total, signal and background PDFs with the weights applied to the signal and background models.}
\label{fig:dWeighted}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{part_d_normalised.png}
\caption{Visualisation of the total, signal and background PDFs, with each distribution properly normalised.}
\label{fig:dNorm}
\end{figure}

\subsection*{Part (c)}

We implemented the normalised signal, background and total PDFs as found in part (b) using the \texttt{scipy.stats} package. We then integrated the total PDF over $[5, 5.6]$ for 1000 random combinations of the parameters, $\boldsymbol{\theta}$, and all integrals came out to unity as expected. 

\subsection*{Part (d)}

We created two plots visualising the distributions with the true parameters. Fig. \ref{fig:dWeighted} includes the weights $f$ and $(1-f)$ on the signal and background distributions. Fig. \ref{fig:dNorm} shows the distributions properly normalised (ie. without the weights).  

\subsection*{Part (e)}

To generate samples from the total PDF, we used the inverse CDF method, which works by generating uniform random numbers in [0, 1] and passing them into the inverse CDF (A.K.A percentage point function, PPF) to generate events distributed by the original PDF. Since we did not have access to the PPF of the total distribution, we implemented an algorithm which, for each event, chooses whether to generate from the signal-only or background-only distributions with probability 0.1 and 0.9, then generates the event using the PPF of the chosen model. This method is significantly more efficient than the accept-reject method, since there are no wasted samples. We used the PPFs of the normal and exponential distributions available in \texttt{scipy.stats}, which are not automatically normalised over $[\alpha, \beta]$, hence rather than generating uniform random numbers in the interval [0, 1], we generated from the interval $[F(\alpha), F(\beta)]$, where $F$ is the CDF of the distribution we are generating from. This guarantees that data will only be generated in the desired range, and it will be distributed by the correctly normalised PDF (since only the relative probability of two points in the range $[\alpha, \beta]$ matters, which the same for a non-normalised model).

We generated a sample of 100K events, then fitted the total PDF to this data using maximum likelihood estimation of the parameters. That is, we estimated the parameters by minimising the negative log likelihood:
\begin{equation}
l
= -ln\left(\prod_i p(M_i; \boldsymbol{\theta})\right)
= -\sum_i ln(p(M_i; \boldsymbol{\theta}))
\label{eq:nll}
\end{equation}

For a given maximum likelihood estimate $\hat{\theta} \in \{\hat{f}, \hat{\lambda}, \hat{\mu}, \hat{\sigma} \}$, the variance of that estimate is given by the minimum variance bound (AKA the Cram√©r-Rao lower bound) in the asymptotic limit of large sample sizes ($N \to \infty$):

\begin{equation}
V(\hat{\theta}) 
= \left( {NE\left[\frac{{\partial^2}l}{{\partial\theta^2}}\right]} \right)^{-1},
\label{eq:varbound}
\end{equation}

where $n$ is the sample size, $E[\cdot]$ is the expectation operator, $l$ is the negative log likelihood. A useful property of maximum likelihood estimation is that the expectation of the double differential of the log likelihood is equal to the evaluation of the double differential at the estimated value. Hence:

\begin{equation}
V(\hat{\theta}) = \left(N\left[\frac{{\partial^2}l}{{\partial\theta^2}}\right]_{\theta=\hat{\theta}} \right)^{-1}
\label{eq:minvar}
\end{equation}

Since we are dealing with a sample of 100K events, this asymptotic limit is a reasonable assumption. In this limit, the maximum likelihood estimate produces an estiamte which is normally distributed, unbiased, consistent and maximally efficient (minimum variance as discussed). This estimation was done using \texttt{iminuit}, using starting parameters as the true parameters shifted by an appropriate random shift. We obtained the following estimates:
\[ \hat{f} = 0.0998 \pm 0.0016, \quad \hat{\lambda} = 0.470 \pm 0.019, \quad \hat{\mu} = 5.27980 \pm 0.00033, \quad \hat{\sigma} = 17.99 \times 10^{-3} \pm 0.32 \times 10^{-3}. \]

For all estimates, the true values of the parameters lie within the uncertainties of the estimates. We then binned and plotted the sample, and overlaid the fitted PDF, as can be seen in Fig. \ref{fig:e}.

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{part_e.png}
\caption{Visualisation of the our 100K sample generated from the true PDF, overlaid by the fitted PDFs of the total, signal and background distributions, whose parameters were estimated via the maximum likelihood method}
\label{fig:e}
\end{figure}

% \end{comment}
\begin{comment}

\section*{Section B}

\subsection*{Introduction}

In section B we consider the theory, implementation and results of part (f) and (g). The goal of part (f) was to estimate the size of a dataset sampled from a 'signal plus background' model, which would lead to a 'discovery' of the signal at least 90\% of the time when we perform a hypothesis test on it. Similarly, the goal of part (g) was to estimate the size of a dataset sampled from a 'two signals plus background' model, which is needed to discover the two distinct signals at least 90\% of the time. For each part, we investigated 50 dataset sizes in an appropriate range found by trial and error. For each dataset size we repeated the following procedure $n$ times: generate a dataset, fit the null ($H_0$) and alternate ($H_1$) hypothesis distributions using maximum likelihood estimation, and perform a Neyman-Pearson hypothesis test. The number of discoveries for each dataset size is binomially distributed, which allows us to estimate the probability of discovery and uncertainty on this estimate from our experiment. We then fit a third order polynomial to approximate the relationship between $p$ and $N$ in the range of interest. This allows us to predict the critical dataset sizes, as well as their uncertainties using a Monte Carlo simulation.  

\subsection*{Methodology}

\subsubsection*{Neyman-Pearson Test}

The power, $1-\beta$, of a hypothesis test is the probability of correctly rejecting the null hypothesis when it is false. The Neyman-Pearson Lemma states that the test statistic which maximises the power is given by the ratio of the likelihoods under each hypothesis:

\begin{equation}
T = -2ln\left( \frac{L(\vec{X}|H_0)}{L(\vec{X}|H_1)} \right)
\label{eq:test_statistic}
\end{equation}

According to Wilk's theorem, in the asymptotic limit of large sample sizes $N \to \infty$, this test statistic under the null hypothesis is $\chi^2$ distributed with 1 degree of freedom. We assume that this asymptotic limit is valid, which allows us to find the p value of the hypothesis test from:
$$
p = 1 - F_{\chi^2, 1}(T),
$$
where $F_{\chi^2, 1}$ is the cumulative density function of the $\chi^2$ distribution with 1 degree of freedom. Since we cannot have a 'negative' presence of signal (ie. the fraction of signal, $f \geq 0$), this is a one-sided test, and we calculate the significance via the formula:
$$
Z = \Phi^{-1}(1 - p)
$$
where $\Phi$ is the cumulative density function of the normal distribution, and $p$ is the p-value of the test. The threshold for a 'discovery' is $Z \geq 5$

\subsubsection*{Binomial Distribution}

In part (f) and (g), on each dataset size, $N_i$, we conduct hypothesis tests on $n$ generated datasets and find $k_i$ discoveries. Each hypothesis test has exactly two mutually exclusive outcomes (discovery or no discovery) and the probability of discovery is constant for constant dataset size, hence the experiment can be thought of as a series of Bernoulli trials, and the number of discoveries is binomially distributed. Let $p_i$ be the true probability of discovery for dataset size $N_i$. The mean of the binomial distribution is $\mu = E[k_i] = p_in$ and the variance is $V(k_i) = np_i(1-p_i)$.

We can estimate the probability of discovery as the frequency of discovery in the experiment:
\begin{equation}
\hat{p}_i = \frac{k_i}{n}
\label{eq:prob_est}
\end{equation}

The variance of this estimate is $V(\hat{p}_i) = V(\frac{k_i}{n}) = \frac{V(k_i)}{n^2} = \frac{p_i(1-p_i)}{n}$, hence we can estimate the uncertainty using:

\begin{equation}
\hat{\sigma_i} = \sqrt{\frac{\hat{p}_i(1-\hat{p}_i)}{n}}
\label{eq:sig_est}
\end{equation}

In general, an estimate, $\hat{\theta}$, is consistent if $\hat{\theta} \to \theta$ as $n \to \infty$. The law of large numbers states that the frequency of an outcome $h = \frac{k}{n}$ converges to the probability of that outcome in the limit of infinite experiments: $\frac{k}{n} \to p$ as $n \to \infty$. Hence, our estimate $\hat{p}_i$ is consistent: $\hat{p}_i = \frac{k_i}{n} \to p_i$ as $n \to \infty$

An estimate, $\hat{\theta}$, is unbiased if $E[\hat{\theta}] = \theta$. The expectation of our estimate is $E[\hat{p}_i] = E[\frac{k_i}{n}] = \frac{E[k_i]}{n} = \frac{\mu}{n} = p_i$, hence this estimate is unbiased.

Since $\hat{p}_i \to p_i$ as $n \to \infty$, we see that also $\hat{\sigma_i} = \sqrt{\frac{\hat{p}_i(1-\hat{p}_i)}{n}} \to \sqrt{\frac{p_i(1-p_i)}{n}} = \sigma_i$ as $n \to \infty$, hence our uncertainty estimate is also consistent (although it is biased).

According to the central limit theorem, our estimate $\hat{p}_i = \frac{k_i}{n}$ is normally distributed in the limit of large $n$.

\subsubsection*{Least Squares Fitting}

Once we have collected our data $\{(N_i, \hat{p}_i \pm \hat{\sigma_i})\}$, we need to predict the size of the dataset at which the probability of discovery is 90\%, $N_{90}$. To make this prediction, we can fit some model, $f(N; \boldsymbol{\theta})$, to our data by minimising the 'chi-squared' with respect to the parameters of the model $\boldsymbol{\theta}$, given by:

\begin{equation}
\chi^2 = \sum_i^{50} \frac{(\hat{p}_i - f(N_i| \boldsymbol{\theta}))^2}{\hat{\sigma_i}^2}.
\label{eq:chisq}
\end{equation}

If our probability estimates $\hat{p}_i$ are normally distributed with mean $f(N_i| \hat{\boldsymbol{\theta}})$ and standard deviation $\hat{\sigma_i}$, then $\chi^2$ is proportional to the negative log likelihood \ref{eq:nll}. Hence, under the assumption that $\hat{p}_i$ is normally distributed (which is valid in the limit of large $n$), least squares fitting is equivalent to maximum likelihood estimation. As discussed in part (e), if our sample size of $\hat{p}_i$ is large, then the maximum likelihood estimates are maximally efficient, and so we can estimate the errors of the fitted model parameters using the minimum variance bound \ref{eq:varbound}. However, the asymptotic limit may not be valid as we are dealing with only 50 data points. 

Additionally, if this assumption is valid, then the fitted model should make predictions that are within the uncertainties of the measurements 68.3\% of the time (since one standard deviation of the normal distribution contains 68.3\% of the total probability). 

\subsubsection*{$\chi^2$ test}

A useful method to evaluate goodness-of-fit is a $\chi^2$ test. We calculate $\chi^2$ using equation \ref{eq:chisq}. If our probability estimates $\hat{p}_i$ are normally distributed with mean $f(N_i| \hat{\boldsymbol{\theta}})$ and standard deviation $\hat{\sigma_i}$, then \ref{eq:chisq} is $\chi^2$ distributed with $k = n_{obs} - n_{params}$ degrees of freedom, where $n_{obs}$ is the number of observations and $n_{params}$ is the number of parameters of the fitted model. The expectation value of the $\chi^2$ distribution is $k$, hence on average, we expect $\chi^2/d.o.f \approx 1$. The p-value for this test is the probability that we obtain a $\chi^2$ as large or larger than the value we measured. This can obtained using the formula:

\begin{equation}
p = 1 - F_{\chi^2, k}(\chi^2),
\label{pval_chi2}
\end{equation}
where $F_{\chi^2, k}$ is the cumulative density function of the $\chi^2$ distribution with $k$ degrees of freedom. 

The higher the p-value, the better the fitted model agrees with the data. Conventionally, a p-value of less than 5\% is considered statistically significant, indicating that the model is not a good fit for the data. A p-value which is too high may indicate that the model has over-fitted to the data.

\subsubsection*{Monte Carlo Simulation}

Once we have a model $f(N| \hat{\boldsymbol{\theta}})$ with parameter estimates $\hat{\boldsymbol{\theta}}$ and uncertainties on those parameter estimates, we can estimate the critical dataset size by finding $N_{90}$ for which $f(N_{90}| \hat{\boldsymbol{\theta}}) = 0.9$. 

Since the parameter estimates for our model were obtained by least squares fitting, which is equivalent to maximum likelihood estimation in the limit of large $n$, the parameter estimates are normally distributed (assuming this limit is valid). Thus, we can estimate the uncertainty of $N_{90}$ via a monte carlo simulation, in which we sample parameters $\theta_i$ from a normal distribution with mean $\hat{\theta_i}$ and standard deviation being the error on $\hat{\theta_i}$, then solve for $N_{90}$. This gives us a sample of measurements of $N_{90}$, from which we can estimate the error on $N_{90}$ as the sample standard deviation.

\subsection*{Part (f)}

We found the range of dataset sizes over which to conduct the experiment via a trial and error approach. We repeatedly generated datasets for a range of sizes, initially on a logarithmic scale, and conducted hypothesis tests on them. This provided loose estimates of the probability of discovery, and allowed us to find the range over which the probability increases from 0\% to 100\%. After repeating this on different ranges, we determined that the desirable range to perform the experiment is $N \in [500, 900]$. This was chosen as it appeared to contain the true $N_{90}$, and it is narrow enough to make a precise measurement of $N_{90}$. For 50 equally-spaced sizes in this range, we generated $n = 500$ example datasets. 

For each dataset, we conducted a hypothesis test for the existence of a signal. The null ($H_0$) and alternate ($H_1$) hypotheses were constructed by fitting a 'background-only model' (for $H_0$) and a 'signal plus background' model (for $H_1$) to the dataset using maximum likelihood estimation (as described in part (e)). We chose to use the Neyman-Pearson test statistic \ref{eq:test_statistic} as it is the most powerful test, hence it maximises the probability of discovering the signal. The outcomes of the tests allowed us to estimate the probability of discovery, $\hat{p}_i$, for each size, $N_i$, and the corresponding uncertainty, $\hat{\sigma_i}$, using equations \ref{eq:prob_est} and \ref{eq:sig_est} respectively. 

We then fitted a third order polynomial, $f(N)$, to the dataset $\{(N_i, \hat{p}_i \pm \hat{\sigma_i})| i = 1, ..., 50\}$ using least squares fitting \ref{eq:chisq}. We chose a third order polynomial as it is sufficient to approximate the relationship between $p$ and $N$ within the range of interest. It should be noted that on a wider range of dataset sizes, the relationship is S-shaped (moves from $p=0$ at low $N$ to $p=1$ at high $N$), and cannot be described using a third order polynomial. 

If we assume that $\hat{p}_i$ is normally distributed with mean $f(N_i)$ and standard deviation $\hat{\sigma_i}$, then least squares fitting is equivalent to maximum likelihood estimation, and we can estimate the uncertainties of the fitted model as the minimum variance bound (equation \ref{eq:varbound}). Due to the central limit theorem, the assumption that $\hat{p}_i$ is normally distributed is valid in the limit of large $n$, but since $n = 500$, it is not certain that this assumption holds. The assumption that the  mean of $\hat{p}_i$ is $f(N_i)$ holds only if the fitted model actually captures true relationship between $N$ and $p$ (ie. It is a good fit). To assess the validity of these assumptions, we calculate that $\text{coverage} = 70\%$ and perform a $\chi^2$ test, in which we calculate $\chi^2/d.o.f = 1.15$ and $\text{p-value} = 21.9\%$. If these assumptions are correct, then we should get $\text{coverage} \approx68.3\%$, $\chi^2/d.o.f \approx 1$, and $\text{p-value} \geq 5\%$. These conditions are all satisfied to a reasonable degree, hence we can make these assumptions safely. 

In Fig. \ref{fig:f}, we plotted the probability of discovery against the size of the dataset, with the fitted model overlaid. Below this plot, there is a plot of the pulls, which are given by:

\begin{equation}
\text{pull} = \frac{\hat{p}_i - f(N_i)}{\hat{\sigma_i}}
\label{eq:pull}
\end{equation}
where $f$ is the fitted polynomial. If the Gaussian assumption on $\hat{p}_i$ is correct and the fitted model prediction is the mean of its distribution, then the pulls are distributed by the standard normal distribution. On the right hand side of the plot is a density histogram of the pulls, with a standard normal distribution overlaid (the red line). The density histogram appears to agree fairly well with the standard normal distribution, indicating that the fitted model and the Gaussian assumption are both valid.

By performing a Monte Carlo simulation to estimate $N_{90}$ and its uncertainty, we yielded the result:
$$
N_{90} = 603 \pm 3
$$

However, the error on this result may not be trustworthy, as it was derived by taking the fitted parameter uncertainties to be the minimum variance bound. Since this is only valid in the asymptotic limit, and our sample size was only 50, it is not clear that this assumption is reasonable.  

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{part_f.png}
\caption{Plot of the estimated probability of discovering a signal vs the size of the dataset. The size of the dataset at which the probability is 0.9 is indicated by the green axis lines. The bottom plot shows the pulls to provide a visualisation of goodness-of-fit. A density histogram is plotted on the right hand edge of the pull plot, with the standard normal distribution overlaid.}
\label{fig:f}
\end{figure} 

\section*{Part (g)}

To find the desired range of dataset sizes to conduct the experiment, we followed the same trial and error procedure as in part (f), and settled on 50 equally-spaced sizes, $\{N_i\}$, in the range $[1500, 3000]$. For each of these sizes, we generated $n = 300$ example datasets. 

For each dataset, we conducted a hypothesis test for the presence of two distinct signals using the Neyman-Pearson test statistic (equation \ref{eq:test_statistic}), and found $k_i$ discoveries. The null ($H_0$) and alternate ($H_1$) hypotheses were constructed by fitting a 'signal plus background model' (for $H_0$) and a 'two signals plus background' model (for $H_1$) to the dataset using maximum likelihood estimation. We estimated the probability of discovery with equation \ref{eq:prob_est} and its uncertainty with equation \ref{eq:sig_est}. 

We then fitted a third order polynomial, $f(N)$, to the dataset $\{(N_i, \hat{p}_i \pm \hat{\sigma_i})| i = 1, ..., 50\}$ using least squares fitting \ref{eq:chisq}. Similarly to part (f), we test goodness-of-fit by calculating the coverage and performing a $\chi^2$ test. The coverage is 60\%, which is fairly close to the target of 68.3\%, indicating that the Gaussian assumption on $\hat{p}_i$ is reasonable, and the fitted model agrees with the data. Furthermore, the $\chi^2$ test revealed $\chi^2/d.o.f = 1.197$ and $\text{p-value} = 16.96\%$. These metrics indicate that the model is a good fit of the data, since $\chi^2/d.o.f$ is close to 1 and the p value is high enough that we can say the model agrees with the data. In Fig. \ref{fig:g} we plot the probability of discovery vs size of dataset with the fitted model overlaid, and the pulls (equation \ref{eq:pull}) visualised directly below. We can see that the density histogram of the pulls agrees well with the standard normal distribution, which suggests indicates agreement between the data and the model. 

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{part_g.png}
\caption{Plot of the estimated probability of discovering two distinct signals vs the size of the dataset. The size of the dataset at which the probability is 0.9 is indicated by the green axis lines. The bottom plot shows the pulls to provide a visualisation of goodness-of-fit. A density histogram is plotted on the right hand edge of the pull plot, with the standard normal distribution overlaid.}
\label{fig:g}
\end{figure} 

To estimate the size of dataset needed to discover two distinct signals 90\% of the time, $N_{90}$, we can use the Monte Carlo simulation method described in the methodology section. We obtained the result:

$$ N_{90} = 2570 \pm 35 $$

As discussed in part (f), the uncertainty on this result may not be trustworthy, as it was calculated assuming that the predictive model's parameter estimates were maximally efficient \ref{eq:varbound}, but this is only valid with asymptotic sample sizes. Since we had only 50 data-points, this assumption may not be correct. 

\end{comment}

\end{document}
