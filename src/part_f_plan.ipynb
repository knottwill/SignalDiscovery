{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of data\n",
    "\n",
    "Make `generation.py` module containing `generate_from_total_pdf(N_events, f=0.1, ...)`\n",
    "\n",
    "We generate `n` datasets at different values of `N` (size of dataset).\n",
    "\n",
    "For a given `N`, should I run 'pilot simulations', to inform what choice for `n` is needed to get a good estimate of the probability of success. We could decide what we want the size of $\\hat{\\sigma_i}$ to be (roughly), as a percentage of $\\hat{p_i}$, (eg. 5%) and then solve for $n_i$ in the equation:\n",
    "\n",
    "$$\\hat{\\sigma_i} = \\sqrt{\\frac{\\hat{p_i}(1-\\hat{p_i})}{n_i}}$$\n",
    "\n",
    "$$n_i = \\frac{\\hat{p_i}(1-\\hat{p_i})}{\\hat{\\sigma_i}^2}$$\n",
    "\n",
    "Suppose we want the uncertainty to be ~5% of our estimate: $\\hat{\\sigma_i} = 0.05\\hat{p_i}$. Then using the equation above, we get:\n",
    "\n",
    "$$n_i = 400(\\frac{1}{\\hat{p_i}} - 1)$$\n",
    "\n",
    "For $\\hat{\\sigma_i} = k\\hat{p_i}$, we get\n",
    "\n",
    "$$n_i = \\frac{1}{k^2}(\\frac{1}{\\hat{p_i}} - 1)$$\n",
    "\n",
    "We could start off with $n_i = 20$ to get our estimate $\\hat{p_i}$, then use this to get a better $n_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/distributions.py:114: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  signal_factor = 1/(norm.cdf(x=beta, loc=mu, scale=sigma) - norm.cdf(x=alpha, loc=mu, scale=sigma))\n",
      "/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/distributions.py:117: RuntimeWarning: invalid value encountered in multiply\n",
      "  signal_cdf = signal_factor*(norm.cdf(x=M, loc=mu, scale=sigma) - norm.cdf(x=alpha, loc=mu, scale=sigma))\n",
      "/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/distributions.py:115: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  background_factor = 1/(expon.cdf(x=beta, scale=1/lam) - expon.cdf(x=alpha, scale=1/lam))\n",
      "/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/distributions.py:118: RuntimeWarning: invalid value encountered in multiply\n",
      "  background_cdf = background_factor*(expon.cdf(x=M, scale=1/lam) - expon.cdf(x=alpha, scale=1/lam))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{10: 1,\n",
       " 16: 0,\n",
       " 26: 1,\n",
       " 42: 0,\n",
       " 69: 0,\n",
       " 112: 0,\n",
       " 183: 0,\n",
       " 297: 0,\n",
       " 483: 0,\n",
       " 784: 0,\n",
       " 1274: 0,\n",
       " 2069: 0,\n",
       " 3359: 0,\n",
       " 5455: 0,\n",
       " 8858: 0,\n",
       " 14384: 12,\n",
       " 23357: 20,\n",
       " 37926: 20,\n",
       " 61584: 20,\n",
       " 100000: 20}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from generation import generate_from_total_pdf\n",
    "from distributions import total_cdf\n",
    "from hypothesis_test import signal_background_test\n",
    "\n",
    "true_params = {'f': 0.1, 'lam': 0.5, 'mu': 5.28, 'sigma': 0.018}\n",
    "\n",
    "starting_params = true_params\n",
    "\n",
    "Ns = np.logspace(1, 5, num=20).astype(int)\n",
    "\n",
    "n_init = 20\n",
    "\n",
    "successes = []\n",
    "for N_i in Ns:\n",
    "    n_discoveries = 0\n",
    "    for _ in range(n_init):\n",
    "        dataset = generate_from_total_pdf(N_i)\n",
    "\n",
    "        discovery, _, _ = signal_background_test(dataset=dataset, cdf=total_cdf, starting_params=starting_params)\n",
    "\n",
    "        if discovery:\n",
    "            n_discoveries += 1\n",
    "    successes.append(n_discoveries)\n",
    "\n",
    "dict(zip(Ns, successes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis test on each dataset\n",
    "\n",
    "Use Matt's example for the higgs discovery in lecture 16 as guidance.\n",
    "\n",
    "We fit the background only model and total model to the dataset, using MLE (or another estimation method).\n",
    "\n",
    "Calculate the neyman-pearson test statistic (I think `iminuit` can do this automatically).\n",
    "\n",
    "Assume: this test statistic, given the null hypothesis, is chi squared distributed with k=1. Is this a valid assumption? I think its only true in the asymptotic limit of high N. On the other hand, it may he the only way to conduct the hypothesis test, so if N is too small for this assumption to be valid, then N may be too small for discovery.\n",
    "\n",
    "`p = 1 - chi2.cdf(T, 1)`\n",
    "\n",
    "Then its a one sided test so the significance is:\n",
    "\n",
    "`Z = np.sqrt(chi2.ppf(1-2p, 1))`\n",
    "\n",
    "If Z>=5 then we have 'discovered the signal' (success)\n",
    "\n",
    "I will have a module for this: `hypothesis_test.py`\n",
    "\n",
    "#### Statistical justification of the assumption\n",
    "\n",
    "I think I need to justify \"this test statistic, given the null hypothesis, is chi squared distributed with k=1\"\n",
    "\n",
    "This is Wilks' theorem, which states that under certain conditions, this test statistic follows a chi-squared distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100000 events in 0.006503s\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NP_test() missing 1 required positional argument: 'bins'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/part_f_plan.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/part_f_plan.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m N_events \u001b[39m=\u001b[39m \u001b[39m100000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/part_f_plan.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m generate_from_total_pdf(N_events)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/willknott/Desktop/DIS/coursework/pds/wdk24/src/part_f_plan.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m NP_test(dataset\u001b[39m=\u001b[39;49mdataset, cdf\u001b[39m=\u001b[39;49mtotal_cdf, starting_params\u001b[39m=\u001b[39;49mtrue_params)\n",
      "\u001b[0;31mTypeError\u001b[0m: NP_test() missing 1 required positional argument: 'bins'"
     ]
    }
   ],
   "source": [
    "from generation import generate_from_total_pdf\n",
    "from distributions import total_cdf, total_model, background_model\n",
    "from hypothesis_test import NP_test\n",
    "\n",
    "true_params = {'f': 0.1, 'lam': 0.5, 'mu': 5.28, 'sigma': 0.018}\n",
    "\n",
    "N_events = 100000\n",
    "dataset = generate_from_total_pdf(N_events)\n",
    "\n",
    "NP_test(dataset=dataset, cdf=total_cdf, starting_params=true_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial dataset\n",
    "\n",
    "After conducting hypothesis test on all datasets for a given $N_i$, we have essentailly conducted a Bernoulli trial, where the number of discoveries (successes), `r`, is binomially distributed, except the probability of success, $p_N$, is unknown. \n",
    "\n",
    "We estimate $\\hat{p_i} = r/n$, where $n$ is the number of trials\n",
    "\n",
    "And the standard error on this estimate is $\\hat{\\sigma_i} = \\sqrt{\\frac{\\hat{p_i}(1-\\hat{p_i})}{n_i}}$\n",
    "\n",
    "Alternatively could we construct a confidence belt???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p vs N\n",
    "\n",
    "Once we have our dataset of $\\{ N_i, \\hat{p_i} \\pm \\hat{\\sigma_i} \\}$, we can plot $p$ vs $N$, fit a line of best fit.\n",
    "\n",
    "We would need to justify the line we fit. Eg. if we fit a straight line, why would there be a linear relationship between $N$ and $p$?\n",
    "\n",
    "The line of best fit should go through 68.3% of the confidence intervals. We will discuss whether this happens.\n",
    "\n",
    "Predict $N_{discovery}$ (value of $N$ for $p=0.9$), as well as an uncertainty on $N_{discovery}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
